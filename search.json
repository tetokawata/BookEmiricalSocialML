[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R/Pythonによる比較・因果推論・予測研究 (ver 0.11)",
    "section": "",
    "text": "Preface\n\n定量的な比較、（反実仮想）因果推定、予測分析をRによって行う方法を紹介\n経済学におけるデータ分析の大部分は、複数の変数間での関係性の理解・利用を目的としている。 本ページでは、ある結果変数\\(Y\\)と独立変数（群）\\(X=X_1,...,X_L\\)の関係性に焦点を当てる。 また\\(Y\\)と\\(X\\)がともに観察でき、関心のある母集団からランダムサンプリングされたデータが入手出来ている。\n具体的な分析目標を大きく（予測）\\(Y\\)の予測関数の推定、（比較）異なる\\(X\\)間での\\(Y\\)の比較、（因果効果）\\(X\\)の変化が\\(Y\\)に与える因果効果の推定、に大別し、それぞれについて簡単な説明とRのサンプルコードの提供\n\n随時Pythonコードを追加\n\nExample dataとしては、Githubレポジトリ内ExampleDataフォルダーのExample.csvを使用\nSection 2 線形モデルを用いたパラメータ推定\nSection 3 予測モデル推定\nSection 4 セミパラメトリック推定 \\(+\\) 機械学習 による平均差の推定\nSection 5 Treatmentグループ別の記述統計\nSection 6 recipesパッケージを用いた統合的なデータ整備"
  },
  {
    "objectID": "intro.html#tidyverse",
    "href": "intro.html#tidyverse",
    "title": "1  準備",
    "section": "1.1 tidyverse",
    "text": "1.1 tidyverse\n\nデータ整備・可視化等の関数群を提供する（メタ）パッケージ\n\n公式ページ"
  },
  {
    "objectID": "intro.html#nativeなpipe演算子",
    "href": "intro.html#nativeなpipe演算子",
    "title": "1  準備",
    "section": "1.2 nativeなpipe演算子",
    "text": "1.2 nativeなpipe演算子\n\nR version 4.1からpipe演算子が、追加パッケージなしで利用可能になった\n\nTools -> Global option -> Code -> “Use native pipe operator” をチェックする\nCtr + Shift + mがショートカット\n現状、magrittrパッケージが提供するpipe (%>%)に比べて、機能が限定されている\n\npipe演算子：二つの入力X1,X2から出力Yを得る関数fについて、pipe演算子を用いると、Y <- X1 |> f(X2)と書き換えられる\npipe演算子を使わない場合、ある出力結果を入力として用いるためには、複数のobjectを作成する必要があり煩雑\n\n\nn <- rnorm(100) # 標準正規分布から100個値を取得し、nと名付ける\n\nhist(n)　# nを用いてヒストグラムを描画\n\n\n\n\n\npipe演算子を使うと以下のようになる\n\n\nrnorm(100) |> \n  hist()"
  },
  {
    "objectID": "ParameterEstimation.html#設定",
    "href": "ParameterEstimation.html#設定",
    "title": "2  線形モデルによるパラメータの推定",
    "section": "2.1 設定",
    "text": "2.1 設定\n\nRPython\n\n\n\nlibrary(tidyverse)\nlibrary(AER)\nlibrary(estimatr) # Estimation with robust standard error\nlibrary(MatchIt) # Matching for preprocess\n\nData_R <- read_csv(\"ExampleData/Example.csv\")\n\n\nestimatr\nMatchIt\n\n\n\n\nimport pandas as pd\nimport statsmodels.formula.api as smf\n\nData_Python = pd.read_csv('ExampleData/Example.csv')"
  },
  {
    "objectID": "ParameterEstimation.html#sec-ParameterEstimation",
    "href": "ParameterEstimation.html#sec-ParameterEstimation",
    "title": "2  線形モデルによるパラメータの推定",
    "section": "2.2 パラメータの推定",
    "text": "2.2 パラメータの推定\n\n\\(\\tau(x)=\\tau,f(x)=\\beta_0+\\beta_1x_1+...+\\beta_Lx_L\\)と特定化\nサンプル内MSEを最大化するように推定\n線形モデルによる推定は、いくつかの問題がある\n\n異なるグループ間で、\\(X\\)の分布が異なる場合、回帰式の定式化に強く依存する\n一般に平均効果ではなく、加重平均が推計される\nサンプルサイズに比べて、少数のコントロール変数を導入できない\n\n以下ではマッチング法、機械学手法を用いた頑強な推定を目指す\n\n\nRPython\n\n\n\nrobust standard errorを計算するためにestimatrパッケージを利用\nlm_robust関数で推定\n\n\nlm_robust(Price ~ Reform + TradeQ + Size + BuildYear + Distance, # Outcome ~ Treatment + Controls\n          data = Data_R)\n\n                 Estimate  Std. Error    t value      Pr(>|t|)      CI Lower\n(Intercept) -1388.4988461 26.32698942 -52.740510  0.000000e+00 -1440.1030211\nReform          3.8725217  0.43467146   8.909078  5.737306e-19     3.0205116\nTradeQ          0.6242352  0.19779659   3.155945  1.602999e-03     0.2365293\nSize            0.8355270  0.01284035  65.070430  0.000000e+00     0.8103583\nBuildYear       0.6981813  0.01307451  53.400209  0.000000e+00     0.6725537\nDistance       -1.4171745  0.04437330 -31.937548 1.679446e-216    -1.5041516\n                 CI Upper    DF\n(Intercept) -1336.8946710 14787\nReform          4.7245319 14787\nTradeQ          1.0119411 14787\nSize            0.8606957 14787\nBuildYear       0.7238090 14787\nDistance       -1.3301973 14787\n\n\n\n\n\nstatsmodelsパッケージを利用\nheteroscedasticity robust (HC3)を設定\n\n\nresults = smf.ols('Price ~ Reform + TradeQ + Size + BuildYear + Distance', data=Data_Python).fit(cov_type='HC3')\nprint(results.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  Price   R-squared:                       0.425\nModel:                            OLS   Adj. R-squared:                  0.425\nMethod:                 Least Squares   F-statistic:                     1197.\nDate:                Tue, 03 Jan 2023   Prob (F-statistic):               0.00\nTime:                        15:17:06   Log-Likelihood:                -67666.\nNo. Observations:               14793   AIC:                         1.353e+05\nDf Residuals:                   14787   BIC:                         1.354e+05\nDf Model:                           5                                         \nCovariance Type:                  HC3                                         \n==============================================================================\n                 coef    std err          z      P>|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept  -1388.4988     26.334    -52.726      0.000   -1440.113   -1336.885\nReform         3.8725      0.435      8.906      0.000       3.020       4.725\nTradeQ         0.6242      0.198      3.155      0.002       0.236       1.012\nSize           0.8355      0.013     65.048      0.000       0.810       0.861\nBuildYear      0.6982      0.013     53.385      0.000       0.673       0.724\nDistance      -1.4172      0.044    -31.928      0.000      -1.504      -1.330\n==============================================================================\nOmnibus:                    33924.864   Durbin-Watson:                   1.199\nProb(Omnibus):                  0.000   Jarque-Bera (JB):       1076517538.013\nSkew:                          21.520   Prob(JB):                         0.00\nKurtosis:                    1323.863   Cond. No.                     3.28e+05\n==============================================================================\n\nNotes:\n[1] Standard Errors are heteroscedasticity robust (HC3)\n[2] The condition number is large, 3.28e+05. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\n\n\n\n\n2.2.1 RCTデータへの応用\n\n原因変数が完全にランダム化されている場合、因果効果の識別を目的に回帰分析を応用する必要はない\n因果効果の推定の改善、効率性向上、を目的とした線形モデルの利用は議論されてきた\nLin (2013) は、以下のような交差項を導入したモデルを用いることで、平均の差の推定に比べて、漸近的効率性が悪化することはない（同等か改善する）ことを示した\n\n\\[E[Y|D,X]=\\beta_{D}\\times D+\\beta_1\\times X_1+...+\\beta_L\\times X_L\\]\n\\[+\\underbrace{\\beta_{1D}\\times D\\times X_1+...+\\beta_{LD}\\times D\\times X_L}_{交差項}\\]\n\nlm_lin関数で推定可能\n\n\nlm_lin(Price ~ Reform, # Outcome ~ Treatment\n       ~ TradeQ + Size + BuildYear + Distance, # ~ Controls\n       data = Data_R)\n\n                       Estimate Std. Error     t value      Pr(>|t|)\n(Intercept)        38.574938756 0.23419642 164.7119043  0.000000e+00\nReform              3.198398709 0.46416205   6.8906941  5.776551e-12\nTradeQ_c            0.589548934 0.24839614   2.3734223  1.763681e-02\nSize_c              0.836196335 0.01462551  57.1738400  0.000000e+00\nBuildYear_c         0.742123631 0.01496681  49.5846277  0.000000e+00\nDistance_c         -1.393206627 0.05322394 -26.1763134 1.120269e-147\nReform:TradeQ_c     0.172655995 0.37815512   0.4565745  6.479836e-01\nReform:Size_c      -0.004222054 0.03058905  -0.1380250  8.902225e-01\nReform:BuildYear_c -0.154969457 0.03044214  -5.0906230  3.612720e-07\nReform:Distance_c  -0.086574362 0.09590703  -0.9026905  3.667049e-01\n                      CI Lower    CI Upper    DF\n(Intercept)        38.11588462 39.03399290 14783\nReform              2.28858332  4.10821410 14783\nTradeQ_c            0.10266158  1.07643629 14783\nSize_c              0.80752852  0.86486415 14783\nBuildYear_c         0.71278682  0.77146044 14783\nDistance_c         -1.49753218 -1.28888107 14783\nReform:TradeQ_c    -0.56857511  0.91388710 14783\nReform:Size_c      -0.06418041  0.05573630 14783\nReform:BuildYear_c -0.21463984 -0.09529907 14783\nReform:Distance_c  -0.27456407  0.10141535 14783"
  },
  {
    "objectID": "ParameterEstimation.html#sec-Matching",
    "href": "ParameterEstimation.html#sec-Matching",
    "title": "2  線形モデルによるパラメータの推定",
    "section": "2.3 マッチング法による修正",
    "text": "2.3 マッチング法による修正\n\n回帰を行う事前準備としてマッチング法を利用する\n\n重回帰が持つ関数形への依存度を減らせる (Ho et al. 2007)\nMathItパッケージを利用\n\n多数のマッチング法が実装されている\n\n\n2.3.1 Coarsened exact matching\n\nCoarsened exact matching (Iacus, King, and Porro 2012)の実装\n\n連続変数をカテゴリー変数化することで、マッチングできるサンプルサイズを増やすことが期待できる\n\n\n\nfit.m <- matchit(Reform ~ TradeQ + Size + BuildYear + Distance,\n                 data = Data_R,\n                 method = \"CEM\"\n                 )\n\n\nマッチング結果の表示\n\n\nfit.m\n\nA matchit object\n - method: Coarsened exact matching\n - number of obs.: 14793 (original), 9625 (matched)\n - target estimand: ATT\n - covariates: TradeQ, Size, BuildYear, Distance\n\n\n\nSample sizesにて、マッチングできなかったサンプル数（14793の事例中、9625サンプルがマッチングできなかった）が確認できる\nマッチング結果の図示\n\n\nfit.m |> \n  summary() |> \n  plot(abs = FALSE)\n\n\n\n\n\nマッチング結果を変数として含んだデータを作成\n\n\ndf <- match.data(fit.m)\n\n\n“subclass”: マッチングしたグループ\n“weights”：マッチング後の推計に用いるウェイト\nマッチングしたデータを用いた推定\n\n新たに作成されるweight (defaltではweights)を用いた、加重推定で実装\n\n\n\nlm_robust(Price ~ Reform + TradeQ + Size + BuildYear + Distance,\n          df,\n          weights = weights,\n          clusters = subclass)\n\n                 Estimate  Std. Error    t value      Pr(>|t|)      CI Lower\n(Intercept) -1311.7224181 49.24745093 -26.635336  1.958605e-69 -1408.8022390\nReform          4.7327819  0.45785352  10.336891  2.940391e-23     3.8336958\nTradeQ          0.4481538  0.28287157   1.584301  1.144379e-01    -0.1090656\nSize            0.7219818  0.01918252  37.637485 2.430409e-113     0.6842264\nBuildYear       0.6615543  0.02458901  26.904467  1.016361e-70     0.6130877\nDistance       -1.3201228  0.07892931 -16.725382  2.395637e-43    -1.4755318\n                 CI Upper       DF\n(Intercept) -1214.6425972 211.0880\nReform          5.6318679 636.5110\nTradeQ          1.0053732 240.7352\nSize            0.7597372 288.6097\nBuildYear       0.7100210 214.7855\nDistance       -1.1647137 264.7544\n\n\n\n\n2.3.2 Propensity score with subclassification\n\nCoarsened exact matchingでもマッチングできないサンプルが多数出てくる可能性\n\nとくに\\(X\\)が大量にある場合\n\n1次元の距離指標を用いて、マッチングを行う\n\n距離指標としては、Mahalanobis’ Distance、Propensity scoreなど\n\nここではPropensity score \\(p_d(X)\\)を用いる\n\n\\[p_d(X)\\equiv \\Pr[D=d|X]\\]\n\n属性\\(X\\)のユニットの中で、原因変数の値が\\(d\\)である人の割合\n未知の場合、データから推定する必要がある\n推定された傾向スコアを用いたStratification マッチング (Imbens 2015 の推奨)\n-　ロジットにて傾向スコアを推定\n\n\nfit.m <- matchit(Reform ~ TradeQ + Size + BuildYear + Distance,\n                 data = Data_R,\n                 method = \"subclass\",\n                 estimand = \"ATE\"\n                 )\n\n\nマッチング結果\n\n\nsummary(fit.m)\n\n\nCall:\nmatchit(formula = Reform ~ TradeQ + Size + BuildYear + Distance, \n    data = Data_R, method = \"subclass\", estimand = \"ATE\")\n\nSummary of Balance for All Data:\n          Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\ndistance         0.3418        0.2260          0.7825     1.4151    0.2293\nTradeQ           2.4827        2.4422          0.0363     1.0239    0.0101\nSize            49.8096       45.1755          0.2152     0.7953    0.0438\nBuildYear     1993.5456     2003.2632         -0.7800     1.1529    0.1649\nDistance         7.2755        6.9736          0.0740     1.0298    0.0149\n          eCDF Max\ndistance    0.3455\nTradeQ      0.0184\nSize        0.1621\nBuildYear   0.3415\nDistance    0.0337\n\nSummary of Balance Across Subclasses\n          Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\ndistance         0.2576        0.2545          0.0212     1.0285    0.0062\nTradeQ           2.4606        2.4535          0.0063     1.0386    0.0060\nSize            45.5250       46.3742         -0.0394     0.8215    0.0172\nBuildYear     2000.4362     2000.8722         -0.0350     0.9634    0.0093\nDistance         6.9579        7.0579         -0.0245     0.9047    0.0061\n          eCDF Max\ndistance    0.0160\nTradeQ      0.0145\nSize        0.0600\nBuildYear   0.0353\nDistance    0.0151\n\nSample Sizes:\n               Control Treated\nAll           11012.   3781.  \nMatched (ESS) 10508.41 2553.82\nMatched       11012.   3781.  \nUnmatched         0.      0.  \nDiscarded         0.      0.  \n\n\n\nマッチング結果の図示\n\n\nfit.m |> \n  summary() |> \n  plot(abs = FALSE)\n\n\n\n\n\nマッチングしたデータを用いた推定\n\n\nlm_robust(Price ~ Reform + TradeQ + Size + BuildYear + Distance,\n          df,\n          weights = weights,\n          clusters = subclass)\n\n                 Estimate  Std. Error    t value      Pr(>|t|)      CI Lower\n(Intercept) -1311.7224181 49.24745093 -26.635336  1.958605e-69 -1408.8022390\nReform          4.7327819  0.45785352  10.336891  2.940391e-23     3.8336958\nTradeQ          0.4481538  0.28287157   1.584301  1.144379e-01    -0.1090656\nSize            0.7219818  0.01918252  37.637485 2.430409e-113     0.6842264\nBuildYear       0.6615543  0.02458901  26.904467  1.016361e-70     0.6130877\nDistance       -1.3201228  0.07892931 -16.725382  2.395637e-43    -1.4755318\n                 CI Upper       DF\n(Intercept) -1214.6425972 211.0880\nReform          5.6318679 636.5110\nTradeQ          1.0053732 240.7352\nSize            0.7597372 288.6097\nBuildYear       0.7100210 214.7855\nDistance       -1.1647137 264.7544"
  },
  {
    "objectID": "ParameterEstimation.html#sec-Appendix",
    "href": "ParameterEstimation.html#sec-Appendix",
    "title": "2  線形モデルによるパラメータの推定",
    "section": "2.4 付録：Dot-and-Whisker plotによる可視化",
    "text": "2.4 付録：Dot-and-Whisker plotによる可視化\n\nDot-and-Whisker図により点推定量と信頼区間を可視化\n\n\nR\n\n\n\nfit.m <- matchit(Reform ~ TradeQ + Size + BuildYear + Distance,\n                 data = Data_R,\n                 method = \"CEM\"\n                 )\n\ndf <- match.data(fit.m)\n\nResult1 <- lm_robust(Price ~ Reform + TradeQ + Size + BuildYear + Distance,\n            data = df) |> \n  tidy() |> \n  filter(term == \"Reform\"\n         ) |> \n  mutate(Method = \"OLS\")\n\nResult1 |> \n  ggplot(aes(y = term,\n             x = estimate,\n             xmin = conf.low,\n             xmax = conf.high)\n         ) +\n  geom_pointrange() +\n  geom_vline(xintercept = 0) +\n  theme_bw()\n\n\n\n\n\nResult2 <- lm_robust(Price ~ Reform + TradeQ + Size + BuildYear + Distance,\n            data = df,\n            weights = weights,\n            clusters = subclass) |> \n  tidy() |> \n  filter(term == \"Reform\"\n         ) |> \n  mutate(Method = \"Matching + OLS\")\n\nResult1 |> \n  bind_rows(Result2) |> \n  ggplot(aes(y = Method,\n             x = estimate,\n             xmin = conf.low,\n             xmax = conf.high)\n         ) +\n  geom_pointrange() +\n  geom_vline(xintercept = 0) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\nHo, Daniel E., Kosuke Imai, Gary King, and Elizabeth A. Stuart. 2007. “Matching as Nonparametric Preprocessing for Reducing Model Dependence in Parametric Causal Inference.” Political Analysis 15 (3): 199–236. https://doi.org/10.1093/pan/mpl013.\n\n\nIacus, Stefano M., Gary King, and Giuseppe Porro. 2012. “Causal Inference Without Balance Checking: Coarsened Exact Matching.” Political Analysis 20 (1): 1–24. https://doi.org/10.1093/pan/mpr013.\n\n\nImbens, Guido W. 2015. “Matching Methods in Practice: Three Examples.” The Journal of Human Resources 50 (2): 373–419. https://www.jstor.org/stable/24735990.\n\n\nLin, Winston. 2013. “Agnostic Notes on Regression Adjustments to Experimental Data: Reexamining Freedman’s Critique.” The Annals of Applied Statistics 7 (1): 295–318. https://doi.org/10.1214/12-AOAS583."
  },
  {
    "objectID": "Prediction.html#問題設定",
    "href": "Prediction.html#問題設定",
    "title": "3  予測",
    "section": "3.1 問題設定",
    "text": "3.1 問題設定\n\n事前に定義する損失関数の母平均 (Population Risk) を最小化するような、予測関数\\(f(X)\\)の推定を目指す。\n\n以下ではMean squared error(MSE)を損失関数として用いる。確率変数\\(Y,X\\)について予測問題は以下のように定義できる\n\n\n\\[\\min_{f(X)}MSE = \\int_{x}E[(Y_i-f(x))^2|X_i=x]g(x)dx\\]\n\n\\(g(x)\\) は\\(x\\)の分布関数\n\n一般に以下の最適化問題の解と一致\n\n\n\\[\\min_{f(X_i)} \\int_{x}E[(\\mu(x)-f(x))^2|X_i=x]g(x)dx\\]\nただし \\(\\mu(x)=E[Y_i|X_i=x]\\)\n\n上記問題を具体的に解くアルゴリズムとして、ここでは OLS, LASSO, Random Forest、およびそれらのSuperLearnerを実装する。"
  },
  {
    "objectID": "Prediction.html#実装",
    "href": "Prediction.html#実装",
    "title": "3  予測",
    "section": "3.2 実装",
    "text": "3.2 実装\n\n以下のPipelineを実装\n\n\n\n\n\nflowchart TB\n  A[データとパッケージの読み込み] --> B[予測タスクの定義/サンプル分割]\n  B --> C1[BuildIn 推定アルゴリズムの定義]\n  B --> C2[+ 前処理]\n  B --> C3[+ パラメータTuning]\n  C1 --> C4[Super Learnerの定義]\n  C2 --> C4[Super Learnerの定義]\n  C3 --> C4[Super Learnerの定義]\n  C1 --> D[Trainデータのみを用いたベンチマーク]\n  C2 --> D[Trainデータのみを用いたベンチマーク]\n  C3 --> D[Trainデータのみを用いたベンチマーク]\n  C4 --> D[Trainデータのみを用いたベンチマーク]\n  D --> E[最善の予測モデルを用いた最終推計]\n  E --> F[Testデータによる評価]\n\n\n\n\n\n\n\n\n\n3.2.1 パッケージ & データ\n\nここではpipelinesによるデータ整備は行わない (暫定的)\n追加でlgr (表示するメッセージを操作), future (並列計算) パッケージを使用\n\n\nlibrary(tidyverse)\nlibrary(mlr3verse) # 機械学習のメタパッケージ\nlibrary(mlr3pipelines) # Stacking用\n\nRaw <- read_csv(\"ExampleData/Example.csv\")\n\nset.seed(123)\n\n\n\n3.2.2 推定タスクの定義\n\n分割数、繰り返し計算回数は最小限に設定\n\n実戦では増やす\n\n\n\nTask <- as_task_regr(Raw, target = \"Price\") # Task設定\n\nGroup <- partition(Task, ratio = 0.8) # Train/Test分割\n\nR2 <-  msr(\"regr.rsq\") # R2で評価\n\nCV <- rsmp(\"cv\",folds = 2) # 2分割交差検証\n\nTerminal = trm(\"evals\",\n               n_evals = 20) # 20回の繰り返し評価\n\nTuner <- tnr(\"grid_search\",\n             resolution = 20) # 20回のグリッドサーチ\n\n\n\n3.2.3 使用するBuildIn Algorithmを定義\n\nSimpleOLS <- lrn(\"regr.lm\", id = \"SimpleOLS\") # OLS\n\nRandomForest <- lrn(\"regr.ranger\", id = \"RandomForest\") # RandomForest\n\n\n\n3.2.4 PreProcess\n\n線形モデルについて、連続変数二乗項と交差項を導入\n\nDuflo のおすすめ\n\n\n\nMutate = po(\"mutate\") # データ加工\nMutate$param_set$values$mutation = list(\n  Size2 = ~ Size*Size,\n  TradeQ2 = ~ TradeQ*TradeQ,\n  BuildYear2 = ~BuildYear*BuildYear,\n  Distance2 = ~Distance*Distance,\n  Size_TradeQ = ~Size*TradeQ,\n  Size_BuildYear = ~Size*BuildYear,\n  Size_Distance = ~ Size*Distance,\n  TradeQ_BuildYear = ~TradeQ*BuildYear,\n  TradeQ_Distance = ~TradeQ*Distance,\n  BuildYear_Distance = ~BuildYear*Distance\n) # 二乗項と交差項の作成\n\nScale = po(\"scale\") # 標準化\n\nOLS <- Mutate %>>% \n  Scale %>>% \n  lrn(\"regr.lm\") |> \n  as_learner() # 二乗項と交差項を導入したOLS\n\nOLS$id <- \"OLS\"\n\n\n\n3.2.5 チューニング付き推定方法の定義\n\nTree、LASSO アルゴリズムについて、HyperParameterを交差検証により推定する\n探索するHyperParameterの範囲を設定する必要がある\n\nmlr3tuningspacesパッケージが提供するおすすめ範囲を使用\n\n\n\nTree <- lrn(\"regr.rpart\") |> \n  lts()\n\nTree <- AutoTuner$new(\n  learner = Tree,\n  resampling = CV,\n  measure = R2,\n  terminator = Terminal,\n  tuner = Tuner\n  )\n\nTree$id <- \"Tree\"\n\nLASSO <- lrn(\"regr.glmnet\") |> \n  lts()\n\nLASSO <- Mutate %>>% \n  Scale %>>% \n  LASSO |> \n  as_learner()\n\nLASSO <- AutoTuner$new(\n  learner = LASSO,\n  resampling = CV,\n  measure = R2,\n  terminator = Terminal,\n  tuner = Tuner\n  )\n\nLASSO$id <- \"LASSO\"\n\n\n\n3.2.6 SuperLearnerの定義\n\nBaseLearner <- list(\n  OLS,\n  RandomForest,\n  Tree,\n  LASSO\n) # 個別推定アルゴリズム\n\nSuperLearner <- lrn(\"regr.lm\",\n                    id = \"SuperLearner\") # 個別予測をまとめるアルゴリズム\n\nStacking <- pipeline_stacking(BaseLearner, \n                              SuperLearner,\n                              use_features = FALSE) |> \n  as_learner() # SuperLearnerの定義\n\nStacking$id <- \"Stacking\"\n\n\n\n3.2.7 ベンチマーク・テスト\n\nDesign <- benchmark_grid(\n  tasks = list(Task$clone()$filter(Group$train)),\n  learners = list(\n    OLS,\n    SimpleOLS,\n    LASSO,\n    RandomForest,\n    Tree,\n    Stacking\n  ),\n  resamplings = CV\n)\n\nlgr::get_logger(\"mlr3\")$set_threshold(\"error\") # Errorのみを表示\nlgr::get_logger(\"bbotk\")$set_threshold(\"error\") # Errorのみを表示\nfuture::plan(\"multisession\") # 並列処理\n\nBenchMark <- benchmark(Design)\n\nBenchMark$aggregate(R2)\n\n   nr      resample_result task_id   learner_id resampling_id iters  regr.rsq\n1:  1 <ResampleResult[21]>     Raw          OLS            cv     2 0.4689119\n2:  2 <ResampleResult[21]>     Raw    SimpleOLS            cv     2 0.4239997\n3:  3 <ResampleResult[21]>     Raw        LASSO            cv     2 0.4450566\n4:  4 <ResampleResult[21]>     Raw RandomForest            cv     2 0.4799319\n5:  5 <ResampleResult[21]>     Raw         Tree            cv     2 0.4375646\n6:  6 <ResampleResult[21]>     Raw     Stacking            cv     2 0.4849559\n\n\n\nRandomForestとStackingはComparableな性能\n\nとりあえずStackingを採用\n\n\n\n\n3.2.8 最終モデル\n\nStakingを全TrainingDataで推定\nTestDataで評価\n\n\nStacking$train(Task, row_ids = Group$train)\nStacking$predict(Task, Group$test)$score(R2)\n\n regr.rsq \n0.6216389"
  },
  {
    "objectID": "CausalMachineLearning.html#設定",
    "href": "CausalMachineLearning.html#設定",
    "title": "4  セミパラメトリック推定",
    "section": "4.1 設定",
    "text": "4.1 設定\n\nRPython\n\n\n\nlibrary(tidyverse)\nlibrary(recipes)\nlibrary(mlr3verse)\nlibrary(mlr3pipelines)\nlibrary(data.table)\nlibrary(DoubleML)\n\nData_R <- fread(\"ExampleData/Example.csv\")\n\nTask_R <- double_ml_data_from_data_frame(Data_R,\n                                         x_cols = c(\"TradeQ\", \"Size\", \"BuildYear\", \"Distance\"),\n                                         y_col = c(\"Price\"),\n                                         d_cols = c(\"Reform\"))\n\n\n\n\nimport pandas as pd\nfrom sklearn.base import clone\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import RandomForestClassifier\nfrom doubleml import DoubleMLData\nfrom doubleml import DoubleMLPLR\nfrom doubleml import DoubleMLIRM\n\nData_Python = pd.read_csv('ExampleData/Example.csv')\n\nTask_Python = DoubleMLData(Data_Python, \n                          y_col = 'Price',\n                          d_cols = 'Reform',\n                          x_cols = ['TradeQ',\"Size\",\"Distance\",\"BuildYear\"])"
  },
  {
    "objectID": "CausalMachineLearning.html#平均効果の推定-partial-linear-model",
    "href": "CausalMachineLearning.html#平均効果の推定-partial-linear-model",
    "title": "4  セミパラメトリック推定",
    "section": "4.2 平均効果の推定: Partial Linear Model",
    "text": "4.2 平均効果の推定: Partial Linear Model\n\n部分線形モデル (Robinson 1988)\nRandomForestとOLSのStackingを用いる\n\nPythonについて、現状、RandomForestのみ\n\n\n\nRPython\n\n\n\nRegOLS <- lrn(\"regr.lm\",\n  id = \"RegressionOLS\"\n)\n\nRegRF <- lrn(\"regr.ranger\",\n  id = \"RegressionRandomForest\"\n)\n\nRegLearners <- list(\n  RegOLS,\n  RegRF\n)\n\nRegSuperLearner <- lrn(\"regr.lm\",\n                    id = \"RegressionSuperLearner\")\n\nRegNuisanceLearner <- \n  pipeline_stacking(RegLearners, RegSuperLearner) |> \n  as_learner()\n\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\n\nFitPLR_R <- DoubleMLPLR$new(Task_R,\n                            ml_l=RegNuisanceLearner$clone(), \n                            ml_m=RegNuisanceLearner$clone(),\n                            n_folds = 2)\n\nFitPLR_R$fit()\n\nprint(FitPLR_R)\n\n================= DoubleMLPLR Object ==================\n\n\n------------------ Data summary      ------------------\nOutcome variable: Price\nTreatment variable(s): Reform\nCovariates: TradeQ, Size, BuildYear, Distance\nInstrument(s): \nNo. Observations: 14793\n\n------------------ Score & algorithm ------------------\nScore function: partialling out\nDML algorithm: dml2\n\n------------------ Machine learner   ------------------\nml_l: RegressionOLS.RegressionRandomForest.nop.featureunion.RegressionSuperLearner\nml_m: RegressionOLS.RegressionRandomForest.nop.featureunion.RegressionSuperLearner\n\n------------------ Resampling        ------------------\nNo. folds: 2\nNo. repeated sample splits: 1\nApply cross-fitting: TRUE\n\n------------------ Fit summary       ------------------\n Estimates and significance testing of the effect of target variables\n       Estimate. Std. Error t value Pr(>|t|)    \nReform    4.8548     0.4007   12.12   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nFitPLR_Python = DoubleMLPLR(Task_Python,\n                            RandomForestRegressor(n_estimators = 500),\n                            RandomForestRegressor(n_estimators = 500),\n                            n_folds = 2)\n\nFitPLR_Python.fit()\n\n<doubleml.double_ml_plr.DoubleMLPLR object at 0x13f400040>\n\nprint(FitPLR_Python)\n\n================== DoubleMLPLR Object ==================\n\n------------------ Data summary      ------------------\nOutcome variable: Price\nTreatment variable(s): ['Reform']\nCovariates: ['TradeQ', 'Size', 'Distance', 'BuildYear']\nInstrument variable(s): None\nNo. Observations: 14793\n\n------------------ Score & algorithm ------------------\nScore function: partialling out\nDML algorithm: dml2\n\n------------------ Machine learner   ------------------\nLearner ml_g: RandomForestRegressor(n_estimators=500)\nLearner ml_m: RandomForestRegressor(n_estimators=500)\n\n------------------ Resampling        ------------------\nNo. folds: 2\nNo. repeated sample splits: 1\nApply cross-fitting: True\n\n------------------ Fit summary       ------------------\n           coef   std err          t         P>|t|     2.5 %    97.5 %\nReform  4.72997  0.416628  11.352974  7.168234e-30  3.913394  5.546546"
  },
  {
    "objectID": "CausalMachineLearning.html#平均効果の推定-aipw",
    "href": "CausalMachineLearning.html#平均効果の推定-aipw",
    "title": "4  セミパラメトリック推定",
    "section": "4.3 平均効果の推定: AIPW",
    "text": "4.3 平均効果の推定: AIPW\n\nAIPW (Robins and Rotnitzky 1995)\n\n\nRPython\n\n\n\nProbOLS <- lrn(\"classif.log_reg\",\n  id = \"ProbLM\",\n  predict_type = \"prob\"\n)\n\nProbRF <- lrn(\"classif.ranger\",\n  id = \"ProbRanger\",\n  predict_type = \"prob\"\n)\n\nProbLearners <- list(ProbOLS,ProbRF)\n\nProbSuperLearner <- lrn(\"classif.log_reg\",\n                    id = \"ProbSuperLearner\")\n\nProbNuisanceLearner <- pipeline_stacking(ProbLearners, ProbSuperLearner) |> \n  as_learner()\n\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\n\nFitAIPW_R = DoubleMLIRM$new(Task_R,\n                            ml_g=RegNuisanceLearner, \n                            ml_m=ProbNuisanceLearner,\n                            n_folds = 2,\n                            trimming_threshold = 0.1)\n\nFitAIPW_R$fit()\n\nprint(FitAIPW_R)\n\n================= DoubleMLIRM Object ==================\n\n\n------------------ Data summary      ------------------\nOutcome variable: Price\nTreatment variable(s): Reform\nCovariates: TradeQ, Size, BuildYear, Distance\nInstrument(s): \nNo. Observations: 14793\n\n------------------ Score & algorithm ------------------\nScore function: ATE\nDML algorithm: dml2\n\n------------------ Machine learner   ------------------\nml_g: RegressionOLS.RegressionRandomForest.nop.featureunion.RegressionSuperLearner\nml_m: ProbLM.ProbRanger.nop.featureunion.ProbSuperLearner\n\n------------------ Resampling        ------------------\nNo. folds: 2\nNo. repeated sample splits: 1\nApply cross-fitting: TRUE\n\n------------------ Fit summary       ------------------\n Estimates and significance testing of the effect of target variables\n       Estimate. Std. Error t value Pr(>|t|)    \nReform     4.035      0.434   9.298   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nFitAIPW_Python = DoubleMLIRM(Task_Python,\n                            RandomForestRegressor(n_estimators = 500),\n                            RandomForestClassifier(n_estimators = 500),\n                            n_folds = 2,\n                            trimming_threshold = 0.1)\n  \nFitAIPW_Python.fit()\n\n<doubleml.double_ml_irm.DoubleMLIRM object at 0x174ed18e0>\n\nFitAIPW_Python.summary\n\n            coef   std err         t         P>|t|     2.5 %    97.5 %\nReform  4.241805  0.567664  7.472381  7.875634e-14  3.129203  5.354407\n\n\n\n\n\n\n\n\n\nChernozhukov, Victor, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, and James Robins. 2018. “Double/Debiased Machine Learning for Treatment and Structural Parameters.” The Econometrics Journal 21 (1): C1–68. https://doi.org/10.1111/ectj.12097.\n\n\nRobins, James M., and Andrea Rotnitzky. 1995. “Semiparametric Efficiency in Multivariate Regression Models with Missing Data.” Journal of the American Statistical Association 90 (429): 122129. https://doi.org/10.1080/01621459.1995.10476494.\n\n\nRobinson, P. M. 1988. “Root-n-Consistent Semiparametric Regression.” Econometrica 56 (4): 931–54. https://doi.org/10.2307/1912705."
  },
  {
    "objectID": "summary.html#設定",
    "href": "summary.html#設定",
    "title": "5  付録: 記述統計",
    "section": "5.1 設定",
    "text": "5.1 設定\n\nRPython\n\n\n\nlibrary(tidyverse)\nlibrary(gtsummary) # 記述統計\nlibrary(cobalt) # バランス確認\n\nData_R <- read_csv(\"ExampleData/Example.csv\")\n\n\ngtsummary : 記述統計表作成\ncobalt : 図作成\n\n\n\n\nimport pandas as pd\nimport seaborn as sns # 可視化\n\n\nData_Python = pd.read_csv('ExampleData/Example.csv')"
  },
  {
    "objectID": "summary.html#sec-Table",
    "href": "summary.html#sec-Table",
    "title": "5  付録: 記述統計",
    "section": "5.2 記述統計表",
    "text": "5.2 記述統計表\n\n\\(D=Reform\\) ごとに以下を表示\n\n\nRPython\n\n\n\n連続変数については、 “中央値(下位25%, 上位25%)”\nカテゴリー変数について、 “サンプルサイズ(割合)”\n\n\nData_R |> \n  tbl_summary(by = \"Reform\")\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      0, N = 11,0121\n      1, N = 3,7811\n    \n  \n  \n    TradeQ\n\n\n        1\n2,864 (26%)\n963 (25%)\n        2\n2,940 (27%)\n967 (26%)\n        3\n2,682 (24%)\n914 (24%)\n        4\n2,526 (23%)\n937 (25%)\n    Size\n45 (25, 65)\n50 (35, 65)\n    BuildYear\n2,006 (1,998, 2,012)\n1,995 (1,984, 2,003)\n    Distance\n6.0 (4.0, 9.0)\n7.0 (4.0, 10.0)\n    Price\n31 (22, 50)\n34 (24, 48)\n  \n  \n  \n    \n      1 n (%); Median (IQR)\n    \n  \n\n\n\n\n\n\n\nData_Python.groupby('Reform').describe().T\n\nReform                      0            1\nTradeQ    count  11012.000000  3781.000000\n          mean       2.442245     2.482677\n          std        1.107117     1.120293\n          min        1.000000     1.000000\n          25%        1.000000     1.000000\n          50%        2.000000     2.000000\n          75%        3.000000     3.000000\n          max        4.000000     4.000000\nSize      count  11012.000000  3781.000000\n          mean      45.175490    49.809574\n          std       22.727207    20.268129\n          min       10.000000    10.000000\n          25%       25.000000    35.000000\n          50%       45.000000    50.000000\n          75%       65.000000    65.000000\n          max      125.000000   125.000000\nBuildYear count  11012.000000  3781.000000\n          mean    2003.263187  1993.545579\n          std       12.008525    12.893772\n          min     1963.000000  1964.000000\n          25%     1998.000000  1984.000000\n          50%     2006.000000  1995.000000\n          75%     2012.000000  2003.000000\n          max     2022.000000  2021.000000\nDistance  count  11012.000000  3781.000000\n          mean       6.973628     7.275513\n          std        4.049774     4.109578\n          min        0.000000     0.000000\n          25%        4.000000     4.000000\n          50%        6.000000     7.000000\n          75%        9.000000    10.000000\n          max       22.000000    21.000000\nPrice     count  11012.000000  3781.000000\n          mean      39.529176    40.086363\n          std       32.085602    27.362758\n          min        0.500000     0.540000\n          25%       22.000000    24.000000\n          50%       31.000000    34.000000\n          75%       50.000000    48.000000\n          max     1600.000000   300.000000"
  },
  {
    "objectID": "summary.html#sec-Histogram",
    "href": "summary.html#sec-Histogram",
    "title": "5  付録: 記述統計",
    "section": "5.3 ヒストグラム",
    "text": "5.3 ヒストグラム\n\n\\(D=Reform\\) ごとに以下を表示\n\n\nRPython\n\n\n\nData_R |> \n  mutate(Reform = factor(Reform)) |> \n  ggplot(\n    aes(\n      x = Size,\n      fill = Reform,\n      group = Reform\n      )\n    ) +\n  geom_histogram(\n    aes(y=..density..),\n    alpha=0.5,\n    position='identity'\n    ) +\n  theme_bw()\n\n\n\n\n\n\n\nsns.displot(\n  Data_Python, \n  x=\"Size\", \n  hue=\"Reform\", \n  stat=\"density\",\n  common_norm=False)"
  },
  {
    "objectID": "summary.html#sec-Figure",
    "href": "summary.html#sec-Figure",
    "title": "5  付録: 記述統計",
    "section": "5.4 バランス確認",
    "text": "5.4 バランス確認\n\n異なるDグループ間で、Xの分布の違いを確認\nXの平均差/Xの標準偏差を報告 (Imbens (2015) などで推奨)\n\n\nRPython\n\n\n\nbal.tab(Reform ~ Size + BuildYear + Distance + TradeQ,\n        Data_R,\n        binary = \"std\", continuous = \"std\") |> \n  plot()\n\n\n\n\n\n築年が古い、広い、駅からの距離が長い物件が、改装されがち\n\n\n\n\nUnder construction\n\n\n\n\n\n\n\n\nImbens, Guido W. 2015. “Matching Methods in Practice: Three Examples.” The Journal of Human Resources 50 (2): 373–419. https://www.jstor.org/stable/24735990."
  },
  {
    "objectID": "PreProcess.html#パッケージ-データ",
    "href": "PreProcess.html#パッケージ-データ",
    "title": "6  付録: データ整備",
    "section": "6.1 パッケージ & データ",
    "text": "6.1 パッケージ & データ\n\nlibrary(tidyverse)\nlibrary(AER)\nlibrary(recipes)\n\ndata(\"CPSSW9204\")\n\n\ntidyverse : 個別変数についてのデータ加工関数群を提供\nrecipes : 統合的なデータ加工関数を提供"
  },
  {
    "objectID": "PreProcess.html#recipes",
    "href": "PreProcess.html#recipes",
    "title": "6  付録: データ整備",
    "section": "6.2 recipes",
    "text": "6.2 recipes\n\nX <- recipe(earnings ~ degree + year + gender + age,\n            CPSSW9204) |> # 推定式を定義\n  update_role(degree,\n              new_role = \"treatment\") |> # degreeをtreatmentに指定\n  step_unknown(all_nominal_predictors()) |> # 非数値変数について、欠損値を\"unknown\"に変更\n  step_other(all_nominal_predictors()) |> # 非数値変数について、小規模グループを\"other\"に変更\n  step_dummy(all_nominal_predictors()) |> # 非数値変数をダミー変数に変換\n  step_indicate_na(all_numeric_predictors()) |> # 数値変数について、欠損ダミーを作成\n  step_impute_median(all_numeric_predictors()) |> # 数値変数について、欠損値に中央値を補完\n  step_nzv(all_numeric_predictors()) |> # 数値変数について、 変動が少ない変数を除外\n  step_corr(all_numeric_predictors()) |> # 相関が強い変数を除去\n  prep() |> \n  bake(CPSSW9204)"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Chernozhukov, Victor, Denis Chetverikov, Mert Demirer, Esther Duflo,\nChristian Hansen, Whitney Newey, and James Robins. 2018.\n“Double/Debiased Machine Learning for Treatment and Structural\nParameters.” The Econometrics Journal 21 (1): C1–68. https://doi.org/10.1111/ectj.12097.\n\n\nHo, Daniel E., Kosuke Imai, Gary King, and Elizabeth A. Stuart. 2007.\n“Matching as Nonparametric Preprocessing for Reducing Model\nDependence in Parametric Causal Inference.” Political\nAnalysis 15 (3): 199–236. https://doi.org/10.1093/pan/mpl013.\n\n\nIacus, Stefano M., Gary King, and Giuseppe Porro. 2012. “Causal\nInference Without Balance Checking: Coarsened Exact Matching.”\nPolitical Analysis 20 (1): 1–24. https://doi.org/10.1093/pan/mpr013.\n\n\nImbens, Guido W. 2015. “Matching Methods in Practice: Three\nExamples.” The Journal of Human Resources 50 (2):\n373–419. https://www.jstor.org/stable/24735990.\n\n\nLin, Winston. 2013. “Agnostic Notes on Regression Adjustments to\nExperimental Data: Reexamining Freedman’s Critique.”\nThe Annals of Applied Statistics 7 (1): 295–318. https://doi.org/10.1214/12-AOAS583.\n\n\nRobins, James M., and Andrea Rotnitzky. 1995. “Semiparametric\nEfficiency in Multivariate Regression Models with Missing Data.”\nJournal of the American Statistical Association 90 (429):\n122129. https://doi.org/10.1080/01621459.1995.10476494.\n\n\nRobinson, P. M. 1988. “Root-n-Consistent Semiparametric\nRegression.” Econometrica 56 (4): 931–54. https://doi.org/10.2307/1912705."
  }
]